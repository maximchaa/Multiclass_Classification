{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "def bold(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'.\\Data\\problem_train.csv')\n",
    "test = pd.read_csv(r'.\\Data\\problem_test.csv')\n",
    "\n",
    "labels = pd.read_csv(r'.\\Data\\problem_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "labels = labels.drop('id', axis=1)\n",
    "\n",
    "\n",
    "train_nas = train.iloc[:, list(train.isnull().sum() / len(train) * 100 == 100)].columns\n",
    "test_nas = test.iloc[:, list(test.isnull().sum() / len(test) * 100 == 100)].columns\n",
    "nas_cols = list(set(train_nas) | set(test_nas))\n",
    "\n",
    "train = train.drop(nas_cols, axis=1)\n",
    "test = test.drop(nas_cols, axis=1)\n",
    "\n",
    "\n",
    "train_dropped = []\n",
    "for col in train.columns:\n",
    "    if (train[col].isnull().sum() <= 1000) & (len(train[col].value_counts()) == 1) or \\\n",
    "        (train[col].isnull().sum() == train.shape[0]):\n",
    "            train_dropped.append(col)\n",
    "            train = train.drop(col, axis=1)\n",
    "            \n",
    "test_dropped = []\n",
    "for col in test.columns:\n",
    "    if (test[col].isnull().sum() <= 1000) & (len(test[col].value_counts()) == 1) or \\\n",
    "        (test[col].isnull().sum() == test.shape[0]):\n",
    "            test_dropped.append(col)\n",
    "            test = test.drop(col, axis=1)\n",
    "            \n",
    "            \n",
    "cols_to_drop = []\n",
    "for col in train.columns:\n",
    "    if len(train[col].value_counts()) == 1:\n",
    "        if train[col].value_counts()[train[col].value_counts().keys()[0]] == train.shape[0]:\n",
    "            cols_to_drop.append(col)\n",
    "print(cols_to_drop)\n",
    "\n",
    "cols_to_drop = []\n",
    "for col in test.columns:\n",
    "    if len(test[col].value_counts()) == 1:\n",
    "        if test[col].value_counts()[test[col].value_counts().keys()[0]] == test.shape[0]:\n",
    "            cols_to_drop.append(col)\n",
    "print(cols_to_drop)\n",
    "            \n",
    "sorted(train.columns) == sorted(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_noNas = list(train.iloc[:, list(train.isnull().any() == False)].columns)\n",
    "test_noNas = list(test.iloc[:, list(test.isnull().any() == False)].columns)\n",
    "noNAs = list(set(train_noNas) & set(test_noNas))\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaps filling: part IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле, восстанавливать плотности была не очень здравой идей изначально, но время было, и мне было интересно.\n",
    "\n",
    "И, вообще говоря, еще до этого помнил про такую штуку, как SVD, отложил и благополучно забыл, а теперь опять вспомнил. Какое счастье, а ведь чуть не забыл окончательно!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular value decomposition\n",
    "\n",
    "**Теорема (SVD)**:\n",
    "<br>Произвольная $l \\times n$-матрица представима в виде $$F = VDU^{T}$$\n",
    "<br>где \n",
    "* $l \\times n$-матрица $V = (v_1, \\ldots, v_n)$ ортонормирована $\\left( V^{T} V = E_n \\right)$, \n",
    "<br>столбцы $v_j$ - собственные векторы матрицы $F F^{T}$;\n",
    "* $n \\times n$-матрицы $D = diag \\left( \\sqrt{\\lambda_n}, \\ldots,\\sqrt{\\lambda_n} \\right)$,\n",
    "<br>$\\lambda_j \\geq 0$ - собственные значения матриц $F F^{T}$ и $F^{T} F$;\n",
    "* $n \\times n$-матрица $U = (u_1, \\ldots, u_n)$ ортонормирована $\\left( Г^{T} Г = E_n \\right)$, \n",
    "<br>столбцы $u_j$ - собственные векторы матрицы $F^{T} F$.\n",
    "\n",
    "<br>---\n",
    "\n",
    "<br>Иногда по разным причинам мы можем хотеть обратить матрицу $F$ (например, при поиске решения множественной линейной регрессии методом МНК. Но обратная не всегда хорошо ищется. Например, у нас могу быть линейно-зависимые столбцы, и тогда обратная матрица вообще не считается, либо у нас могут быть коррелированные столбцы или интразинзитивно коррелированы ($\\rho(A, B), \\ \\rho(B, C) > 0$, но $\\rho(A, C) < 0$).\n",
    "\n",
    "Если у нас есть почти линейно зависимые столбцы в матрице $F$, то некоторые с.зн. (собственные значения) матрицы $F F^{T} \\rightarrow 0$. Если размах максимального и минимального с.зн. составляет порядки, то возникает вычислительная неустойчивость, решение будет плохо интерпретируемым, а также возникнет эффект переобучения (на обучении метрика классная, а на контроле сасная).\n",
    "\n",
    "Это можно побороть, например, через введение регуляризации, которая вместе с этим будет понижать нам размерность, тем самым отбирать признаки.\n",
    "\n",
    "<br>---\n",
    "\n",
    "<br>**Principal component analysis**:\n",
    "<br>Но от мультиколлинеарности можно избавляться не только с помощью регуляризации, но и переходом от имеющихся признаков к каким-то новым. Т.е. произойдет изменение признаково пространства, скорее всего, с понижением его размерности.\n",
    "\n",
    "$f_1(x), \\ldots, f_n(x)$ - исходные *числовые* признаки;\n",
    "<br>$g_1(x), \\ldots, g_m(x)$ - новые *числовые* признаки, $m \\leq n$\n",
    "<br>$+$ *требование*: старые признаки должны линейно восстанавливаться по новым как можно точнее на обучающейся выборке $x_1, \\ldots, x_l$: \n",
    "$$\\hat{f_j}(x) = \\sum\\limits_{s=1}^{m} g_s(x) u_{js}, \\ \\forall x \\in X; \\\\\n",
    "\\sum\\limits_{i=1}^{l} \\sum\\limits_{j=1}^{n} \\left( \\hat{f_j}(x_i) - f_j(x_i) \\right)^{2} \\rightarrow \\min\\limits_{\\{g_s(x_i)\\}, \\{u_{js}\\} }$$\n",
    "Т.е. хотим так подобрать и новые признаки, и их линейные комбинации, чтобы старые признаки восстанавливались как можно более надежно относительно выбранного функционала качества.\n",
    "\n",
    "<br>Введенем понятие *матричной нормы Фробениуса* - сумма квадратов всех элементов матрицы. Будем искать преобразование матрицы в новое подпространство относительно данной нормы: $F \\in M_{l, n}, \\ G \\in M_{l, m}, \\ m \\leq n; \\quad U \\in M_{n, m}$.\n",
    "<br>Хотим: $\\hat{F} = G U^{T} \\approx F$, где $G$ - новые признаки, $U$ - преобразование.\n",
    "\n",
    "<br>---\n",
    "\n",
    "<br>**Теорема (PCA)**:\n",
    "<br>Если $m \\leq rk F$, то минимум $\\left\\lVert G U^{T} - F \\right\\rVert^{2}$ достигается, когда столбцы $U$ - с.в. матрицы $F^{T} F$, соответствующие $m$ максимальным с.зн. $\\lambda_1, \\ldots, \\lambda_m$, <br>а матрица $G = FU$. При этом:\n",
    "* матрица $U$ ортонормирована $\\left( U^{T} U = E_m \\right)$;\n",
    "* матрица G ортогональна: $G^{T} G = \\Lambda = diag(\\lambda_1, \\ldots, \\lambda_m)$;\n",
    "* $U \\Lambda = F^{T} F U; \\quad G \\Lambda = F F^{T} G$;\n",
    "* $\\left\\lVert G U^{T} - F \\right\\rVert^{2} = \\left\\lVert F \\right\\rVert^{2} - tr \\Lambda = \\sum\\limits_{i=m+1}^{n} \\lambda_i$.\n",
    "\n",
    "*Суть*: если возьмем все с.зн. матрицы $F^{T} F$, отсортируем их по убыванию, то первые $m$ из них будут определять наше решение, а последние - ошибку описания.\n",
    "\n",
    "<br>Если $m = n$, то \n",
    "* $\\left\\lVert G U^{T} - F \\right\\rVert^{2} = 0$;\n",
    "* представление $\\hat{F} = G U^{T} = F$ точное и совпаддает с SVD при $G = V \\sqrt{\\Lambda}$: $\\ F = V \\sqrt{\\Lambda} U^{T}$;\n",
    "* линейное преобразование $U$ работает в обе стороны: $F = G U^{T}; \\quad G = F U$.\n",
    "\n",
    "Поскольку новые признаки некоррелированы $\\left( G^{T} G = \\Lambda \\right)$, преобразование $U$ называется *декоррелирующим* (или преобразование *Каруне-Лоэва*).\n",
    "\n",
    "<br>---\n",
    "\n",
    "<br>Будем искать главные компоненты $u_1, \\ldots, u_n$, которые удовлетворяют следующим требованиям:\n",
    "* ортогональность: $\\langle u_i, u_j \\rangle = 0, \\ i != j$;\n",
    "* нормированность: $\\left\\lVert u_i \\right\\rVert^{2} = 1$;\n",
    "* при проецировании выборки на компоненты $u_1, \\ldots, u_m$ получается максимальная дисперсия среди всех возможных способов выбрать $m$ компонент.\n",
    "\n",
    "Чтобы понизить размерность выборки до $m$, будем проецировать ее на первые $m$ компонент -- из последнего свойства следует, что это оптимальный способ снижения размерности. Дисперсия проецированной выборку будет показывать, как много информации удалось сохранить после понижения размерности -- поэтому требуем максимальной дисперсии от проекций.\n",
    "\n",
    "Проекция объекта $x_k$ на компоненту $u_i$ вычисляется как $\\langle x_k , u_i \\rangle$, а проекция всей выборки на эту компоненту - как $X u_i$. Если за $U_m$ обозначить матрицу, столбцы которой равны первым $m$ компонентам, проекция выборки на них будет записываться как $X U_m$, а дисперсия проецированной выборки будет вычисляться как след ковариационной матрицы: $tr U_{m}^{T} X^{T} X U_{m} = \\sum\\limits_{i=1}^{m} \\left\\lVert X u_i \\right\\rVert^{2}.\n",
    "\n",
    "Начнем с первой компоненты. Сведем все требования к ней в оптимизационную задачу:\n",
    "$$\\left\\lVert X u_1 \\right\\rVert^{2} \\rightarrow \\max\\limits_{u_1} \\quad \\& \\quad\n",
    "\\left\\lVert u_1 \\right\\rVert^{2} = 1$$\n",
    "Запишем лагранжиан: \n",
    "$$L(u_1, \\lambda) = \\left\\lVert X u_1 \\right\\rVert^{2} + \\lambda \\left( \\left\\lVert u_1 \\right\\rVert^{2} - 1 \\right)$$\n",
    "Продифференцируем и приравняем к нулю:\n",
    "$$\\frac{\\partial L}{\\partial u_1} = 2 X^{T} X u_1 + 2 \\lambda u_1 = 0$$\n",
    "Отсюда получаем, что $u_1$ должен быть собственным вектором ковариационной матрицы $X^{T} X$. Учтем это и преобразуем функционал:\n",
    "$$ \\left\\lVert X u_1 \\right\\rVert^{2} = u_{1}^{T} X^{T} X u_{1} = \\lambda u_{1}^{T} u_{1} = \\lambda \\rightarrow \\max\\limits_{u_1}$$\n",
    "Значит, собственный вектор $u_1$ должен соответствовать максимальному с.зн.\n",
    "\n",
    "Для следующих компонент к оптимизационной задаче будут добавляться требования ортогональности предыдущим компонентам. Решая эти задачи, мы получим, что главная компонента $u_i$ равна с.в., соответствующему $i$-му с.зн.\n",
    "<br>После того, как найдены главные компоненты, можно проецировать на них данные. При таком способе новые признаки вычисляются как линейные комбинации старых: $z_{ij}^{'} = \\sum\\limits_{k=1}^{n} x_{ik}^{'} u_{kj}$.\n",
    "\n",
    "<br>Если $m$ признаков могут быть выбраны в качестве базиса, а остальные линейное (или почти линейно) через них выражаются, то это будет хорошо видно на спектре (множестве с.зн. матрицы).\n",
    "<br>Пусть $\\lambda_1 \\geq \\ldots \\geq \\lambda_n \\geq 0$. \n",
    "<br>*Эффективная размерность выборки* - это наименьшее целом $m$, при котором\n",
    "$$E_m = \\frac{\\left\\lVert G U^{T} - F \\right\\rVert^{2}}{\\left\\lVert F \\right\\rVert^{2}} = \\frac{\\lambda_{m+1} + \\cdots + \\lambda_n}{\\lambda_1 + \\cdots + \\lambda_n} \\leq \\epsilon$$\n",
    "\n",
    "<br>Так же обратим внимание, что SVD-разложение не единственное. Например, если $A$ квадратная и равна единичной матрице, то подойдет любое разложение вида $A = U E U^{T}$, где $U$ - произвольная ортогональная матрица. В данном случае эффект связан с тем, что все сингулярные значения одинаковые. Но, даже если сингулярные значения разные, всегда можно домножать соответствующие столбцы $U$ и $V$ на $-1$, например: \n",
    "$$\\left( u_1 | \\ldots | u_m \\right) \\Sigma \\left( v_1 | \\ldots | v_n \\right)^{T} = A = \n",
    "\\left( -u_1 | \\ldots | u_m \\right) \\Sigma \\left( -v_1 | \\ldots | v_n \\right)^{T}$$\n",
    "Но по модулю вариантов домножения на много $-1$, в случае с различными собственными значениями, разложение определено однозначно. \n",
    "<br>На самом деле, достаточно, чтобы $\\lambda_m$ и $\\lambda_{m+1}$ были различны.\n",
    "\n",
    "Последний пример так же показывает, что между столбцами $U$ и $V$ есть некоторое условие согласованности. Из-за неоднозначности в разложении надо аккуратно искать все его компоненты: если найденно что-то, например, матрицы $U$ и $\\Sigma$, то матрицу $V$ надо искать не абы каким методом.\n",
    "<br>---\n",
    "\n",
    "<br>**Еще про SVD и PCA**.\n",
    "\n",
    "Допустим, что у нас задан случайный вектор $\\xi \\in {\\rm I\\!R}^{n}$ и мы для него намерили независимо несколько семплов $x_1, \\ldots, x_l \\in {\\rm I\\!R}^{n}$. Первым делом можем оценить математическое ожидание и матрицу ковариации для $\\xi$:\n",
    "$$ {\\rm I\\!E}\\xi = \\overline{x} = \\frac{x_1 + \\cdots + x_l}{l}, \\quad \\Sigma_0 = \\frac{1}{l-1} \\left(x_i - \\overline{x} \\left) \\right(x_i - \\overline{x} \\right)^{t}$$\n",
    "Составим матрицу семплов $X = \\left( x_1 | \\ldots | x_l \\right)$ и сдвинем каждый семпл на выборочное мат ожидание $Y = X - (\\overline{x} | \\ldots | \\overline{x})$.\n",
    "\n",
    "Что будет, если применить к данной матрице SVD? Алгоритм SVD начинается с того, что надо диагонализировать матрицу $Y Y^{T}$:\n",
    "$\\ Y Y^{t} = (x_1 - \\overline{x})(x_1 - \\overline{x}^{t} + \\cdots + (x_l - \\overline{x})(x_l - \\overline{x})^{t}$, что с точностью до коэффициента совпадает с матрицей выборочной ковариации. Значит, диагонализация выборочной ковариации - первый шаг к SVD для $Y$. Интересненько. SVD для Y: $Y = U D V^{T}$.\n",
    "\n",
    "<br>Но продолжим. \n",
    "<br>На SVD можно смотреть вот как: $D = U^{T} Y V$. Т.е. мы стартовали с матрицы из несмещенных семплов Y и теперь с помощью матрицы $U$ пытаемся поменять координаты в пространстве ${\\rm I\\!R}^{n}$, где живут наши семплы, а с помощью матрицы $V$ пытаемся комбинировать наши семплы между собой. Зачастую операция комбинирования семплов не очень физически/геометрически осмысленна, поэтому давайте ее проигнорируем и рассмотрим равенство $U^{T} Y = D V^{T}$. Тогда вот какой смысл у написанного. Мы стартовали с Y. Потом выбрали для выборочной матрицы ковариации $\\Sigma_0$ с.в-ы ортонормированные и сложили их в матрицу $U = \\left( u_1 | \\ldots | u_n \\right)$. Далее привели матрицу $Sigma_0$ к главным осям, т.е. сделали замену стандартного базиса на базис $u_1, \\ldots, u_n$. При этом координаты наших семплов как раз изменятся по правило $ Y \\to U^{T} Y = D V^{T} $. Если расписать это равенство в виде матриц, то увидим, что у ортогональной матрицы $V^{T}$ отрезали верхнюю часть. Так как матрица $V$ была ортогональна, то интуитивно, ее координаты вносят одинаковый вклад в итоговую матрицу. Однако, после этого мы каждую координату домножили на весовой коэффициент $\\lambda_j$, а какие-то из последних даже на $0$. Таким образом после смены координат для несмещенных семплов $Y \\to U^{T} Y$ мы отсортировали координаты по убыванию по их важности, где $\\lambda_j$ означает вес важности координаты. При этом мы видим, если расписать в матричном виде, что последние координаты мы вообще можем проигнорировать, ибо они стали нулевыми. Кромет ого, мы еще можем проигнорировать координаты с малыми $\\lambda_j$.\n",
    "\n",
    "По другому еще можно сказать так. Мы нашли, что все наши семплы жили в подпространстве ${\\rm I\\!R}^{m}$ и после поворота пространства, отрезав лишние координаты, мы можем считать, что наши семплы имеют определенный специфичный вид: $D V^{T}$, где $D \\in M_{m, m}, \\ V \\in M_{n, m}$.\n",
    "\n",
    "<br>И последнее.\n",
    "<br>Пусть $F \\in M_{n, m}$ и ей в соответствие поставлен линейный оператор, также обозначаемый $F$. По SVD: $F = U \\Lambda V^{T}$ -- это можно переформулировать в геометрических терминах. Линейный оператор, отобрадающий элементы пространства ${\\rm I\\!R}^{n}$ в элементы пространства ${\\rm I\\!R}^{m}$ представим в виде последовательно выполняемых линейных операций *вращения, растяжения и вращения*. Число ненулевых элементов на диагонали матрицы $\\Lambda$ есть фактическая размерность матрицы $F$.\n",
    "\n",
    "При этом по растяжением здесь имеется в виду диагональный оператор -- растяжение вдоль стандартного базиса (самосопряженный - вдоль любогго ортонормированного), причем каждую ось тянем со своим коэффициентом. SVD выделяет взаимосвязанные оси и коэффициенты их зависимостей друг с другом. При этом оси выделены явно в виде столбцов $U$ и $V$. А сингулярные значения матрицы $F$ - длины осей эллипсоида, заданного множеством $\\{ A x = \\left\\lVert x \\right\\rVert^{2} \\}$. \n",
    "<br>*Если просто раскладывать на вращение-растяжение, то это полярное разложение, и оно только для квадратных (в отличие от SVD) матриц.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За 5 минут очень легко нашелся пакет в R'е для зафилливания библиотечным путем через SVD, а на Python довольно грустно, кажется. <br>Поэтому переключимся временно на язык R.\n",
    "\n",
    "<br>Здесь подключим зафиллинный уже train, а код на R'е будет в отдельном файле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(r'\\Data\\missing_train.csv',\n",
    "             index=False)\n",
    "\n",
    "train_filled = pd.read_csv(r'\\Data\\filled_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так, ладно. У меня проблема с пакетами: то моя версия слишком старая, то теперь слишком новая, то хочет выделить 21000 Gb памяти. Вернемся в Python.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_svd = train.copy()\n",
    "test_svd = test.copy()\n",
    "sorted(train.columns) == sorted(train_svd.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с выкидывания полностью пустых колонок и зафилливания mean'ом (или median) пропусков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1188)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = train_svd.isnull().sum() == 8000\n",
    "cols_full_na = list(train_svd.columns[mask])\n",
    "\n",
    "mask = test_svd.isnull().sum() == 8000\n",
    "cols_full_na_test = list(test_svd.columns[mask])\n",
    "\n",
    "train_svd = train_svd.drop(cols_full_na, axis=1)\n",
    "train_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>n_0000</th>\n",
       "      <th>n_0001</th>\n",
       "      <th>n_0002</th>\n",
       "      <th>n_0003</th>\n",
       "      <th>n_0004</th>\n",
       "      <th>n_0005</th>\n",
       "      <th>n_0006</th>\n",
       "      <th>n_0007</th>\n",
       "      <th>n_0009</th>\n",
       "      <th>...</th>\n",
       "      <th>c_1366</th>\n",
       "      <th>c_1367</th>\n",
       "      <th>c_1369</th>\n",
       "      <th>c_1370</th>\n",
       "      <th>c_1372</th>\n",
       "      <th>c_1373</th>\n",
       "      <th>c_1374</th>\n",
       "      <th>c_1375</th>\n",
       "      <th>c_1376</th>\n",
       "      <th>c_1377</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>q</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>b</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  release  n_0000  n_0001    n_0002  n_0003  n_0004    n_0005  n_0006  n_0007  \\\n",
       "0       a     0.0     0.0  0.025449     0.0     0.0  0.368421     0.0     0.0   \n",
       "1       a     0.0     0.0  0.031297     0.0     0.0  0.315789     0.0     0.0   \n",
       "2       a     0.0     0.0  0.024475     0.0     0.0  0.342105     0.0     0.0   \n",
       "3       a     0.0     0.0  0.041694     0.0     0.0  0.447368     0.0     0.0   \n",
       "4       c     0.0     0.0  0.038120     0.0     0.0  0.315789     0.0     0.0   \n",
       "\n",
       "   n_0009  ...  c_1366  c_1367  c_1369  c_1370  c_1372  c_1373  c_1374  \\\n",
       "0     0.0  ...                                       a               q   \n",
       "1     0.0  ...                               a       a                   \n",
       "2     0.0  ...                               a       a               b   \n",
       "3     0.0  ...                                       a                   \n",
       "4     0.0  ...                               b       a               a   \n",
       "\n",
       "   c_1375  c_1376  c_1377  \n",
       "0                          \n",
       "1                          \n",
       "2                          \n",
       "3                          \n",
       "4                          \n",
       "\n",
       "[5 rows x 1188 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "num_cols = train_svd.select_dtypes(include=numerics).columns\n",
    "train_svd[num_cols] = train_svd[num_cols].fillna(0)\n",
    "\n",
    "str_cols = train_svd.select_dtypes(include='object').columns\n",
    "train_svd[str_cols] = train_svd[str_cols].fillna('')\n",
    "\n",
    "train_svd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 6491)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_svd_ohe = OneHotEncoder(handle_unknown=\"ignore\").fit_transform(train_svd[str_cols])\n",
    "train_svd_ohe = pd.DataFrame.sparse.from_spmatrix(train_svd_ohe)\n",
    "\n",
    "train_svd = train_svd.select_dtypes(include=numerics)\n",
    "train_svd = pd.concat([train_svd, train_svd_ohe], axis=1, join=\"inner\")\n",
    "train_svd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ищем SVD для нашей почищенной матрицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, s, Vh = sp.linalg.svd(train_svd)\n",
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.43590445e-16 -7.11695179e-14  2.54494260e-02 ...  1.00000000e+00\n",
      "   8.89489194e-15  4.26305066e-15]\n",
      " [ 8.30634389e-16 -2.98741615e-14  3.12973793e-02 ...  1.00000000e+00\n",
      "   1.58788238e-15 -3.76034464e-15]\n",
      " [ 2.04128164e-16 -1.08820965e-14  2.44747672e-02 ...  1.00000000e+00\n",
      "   9.19667717e-16 -6.55122386e-15]\n",
      " ...\n",
      " [-5.64327231e-17  9.04761905e-01  3.34632878e-02 ...  1.00000000e+00\n",
      "   1.35151604e-14  1.04135494e-14]\n",
      " [-4.61243321e-16 -1.47268740e-14  4.71085120e-02 ...  1.00000000e+00\n",
      "   2.86369224e-15  2.25538028e-15]\n",
      " [ 3.02397538e-16  7.61904762e-01  4.94910115e-02 ...  1.00000000e+00\n",
      "   1.93411548e-14  2.03403284e-14]]\n"
     ]
    }
   ],
   "source": [
    "Sigma = np.zeros((train_svd.shape[0], train_svd.shape[1]))\n",
    "\n",
    "Sigma[:train_svd.shape[1], :train_svd.shape[1]] = np.diag(s)\n",
    "\n",
    "train_svd_matr = U.dot(Sigma.dot(Vh))\n",
    "print(train_svd_matr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_0000</th>\n",
       "      <th>n_0001</th>\n",
       "      <th>n_0002</th>\n",
       "      <th>n_0003</th>\n",
       "      <th>n_0004</th>\n",
       "      <th>n_0005</th>\n",
       "      <th>n_0006</th>\n",
       "      <th>n_0007</th>\n",
       "      <th>n_0009</th>\n",
       "      <th>n_0010</th>\n",
       "      <th>...</th>\n",
       "      <th>6204</th>\n",
       "      <th>6205</th>\n",
       "      <th>6206</th>\n",
       "      <th>6207</th>\n",
       "      <th>6208</th>\n",
       "      <th>6209</th>\n",
       "      <th>6210</th>\n",
       "      <th>6211</th>\n",
       "      <th>6212</th>\n",
       "      <th>6213</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_0000  n_0001    n_0002  n_0003  n_0004    n_0005  n_0006  n_0007  n_0009  \\\n",
       "0     0.0     0.0  0.025449     0.0     0.0  0.368421     0.0     0.0     0.0   \n",
       "1     0.0     0.0  0.031297     0.0     0.0  0.315789     0.0     0.0     0.0   \n",
       "2     0.0     0.0  0.024475     0.0     0.0  0.342105     0.0     0.0     0.0   \n",
       "\n",
       "   n_0010  ...  6204  6205  6206  6207  6208  6209  6210  6211  6212  6213  \n",
       "0     0.0  ...   0.0   0.0   0.0   0.0   1.0   0.0   0.0   1.0   0.0   0.0  \n",
       "1     0.0  ...   0.0   0.0   0.0   0.0   1.0   0.0   0.0   1.0   0.0   0.0  \n",
       "2     0.0  ...   0.0   0.0   0.0   0.0   1.0   0.0   0.0   1.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 6491 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_svd.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чорт, ```sp.linalg```, ```np.linalg``` не вычисляют SVD на размерности ниже.\n",
    "<br>sklearn: 'Here we go again'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.07788041e+02, -1.72553499e+00, -1.77310983e+01, ...,\n",
       "         2.85277375e+00,  9.37020409e-01, -1.96549767e+00],\n",
       "       [ 3.00108913e+02,  1.49057653e+01, -2.86652132e+01, ...,\n",
       "         7.33324297e-01, -6.27538923e+00,  5.25523910e+00],\n",
       "       [ 1.38936860e+02,  3.35994699e+01, -1.44792801e+01, ...,\n",
       "        -4.33372406e+00, -1.89300538e+00,  2.47522159e-01],\n",
       "       ...,\n",
       "       [ 3.73760917e+02, -6.44853793e+00, -2.61903158e+01, ...,\n",
       "         2.96505670e+00, -4.79434516e-01,  1.61803180e+00],\n",
       "       [ 8.05934310e+01,  7.31351717e+01,  4.81905283e+01, ...,\n",
       "        -1.31522148e+00,  2.68243567e-01,  2.84171538e+00],\n",
       "       [ 1.36445394e+02,  5.54581119e+01, -3.31549085e+01, ...,\n",
       "        -9.06556466e-01,  5.77548873e-01,  5.32782783e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Тут, по сути, эксперимент, на практике, понятно, n_components определяются, например, через GridSearchCV\n",
    "svd = TruncatedSVD(n_components=40, n_iter=10, random_state=42)  \n",
    "train_svd = svd.fit_transform(train_svd)\n",
    "\n",
    "train_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 40)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_svd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По всей видимости, нам вернулась разреженная матрица."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model I.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "cbc = CatBoostClassifier(verbose=0)\n",
    "\n",
    "models = {\n",
    "    'LR': lr,\n",
    "    'CB': cbc\n",
    "}\n",
    "predicts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0 test logloss = 0.5916\n",
      "LR 1 test logloss = 0.5818\n",
      "LR 2 test logloss = 0.5507\n",
      "LR 3 test logloss = 0.0503\n",
      "LR 4 test logloss = 0.1913\n",
      "LR 5 test logloss = 0.1159\n",
      "LR 6 test logloss = 0.1916\n",
      "LR 7 test logloss = 0.5536\n",
      "LR 8 test logloss = 0.0744\n",
      "LR 9 test logloss = 0.3803\n",
      "LR 10 test logloss = 0.4892\n",
      "LR 11 test logloss = 0.3149\n",
      "LR 12 test logloss = 0.2751\n",
      "LR 13 test logloss = 0.3620\n",
      "\n",
      "LR took 0:00:48.983791\n",
      "---------------\n",
      "LR test multiclass LogLoss = 0.3373\n",
      "\n",
      "\n",
      "CB 0 test logloss = 0.5591\n",
      "CB 1 test logloss = 0.5821\n",
      "CB 2 test logloss = 0.5504\n",
      "CB 3 test logloss = 0.0538\n",
      "CB 4 test logloss = 0.1963\n",
      "CB 5 test logloss = 0.1204\n",
      "CB 6 test logloss = 0.1949\n",
      "CB 7 test logloss = 0.5427\n",
      "CB 8 test logloss = 0.0797\n",
      "CB 9 test logloss = 0.3702\n",
      "CB 10 test logloss = 0.4908\n",
      "CB 11 test logloss = 0.3025\n",
      "CB 12 test logloss = 0.2671\n",
      "CB 13 test logloss = 0.3700\n",
      "\n",
      "CB took 0:05:00.168871\n",
      "---------------\n",
      "CB test multiclass LogLoss = 0.3343\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in range(0, len(models)):\n",
    "    model = list(models.values())[m]\n",
    "    model_name = list(models)[m]\n",
    "        \n",
    "    t0 = datetime.utcnow()\n",
    "    \n",
    "    for K in range(0, labels.shape[1]):\n",
    "        cv_values = cross_val_score(\n",
    "            model,\n",
    "            train_svd,\n",
    "            labels.iloc[:, K],\n",
    "            cv=4,\n",
    "            scoring='neg_log_loss'\n",
    "        )\n",
    "        \n",
    "        predicts.append(-cv_values.max())\n",
    "        print(model_name + \" \" + str(K) + \" test logloss = %.4f\" % predicts[K])\n",
    "            \n",
    "    t1 = datetime.utcnow()\n",
    "    print()\n",
    "    print(model_name + \" took {}\".format(t1 - t0))\n",
    "    \n",
    "    print('---------------')\n",
    "    print(model_name + ' test multiclass LogLoss = %.4f' % (sum(predicts) / len(predicts)))\n",
    "    print('\\n')\n",
    "    predicts.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшее было:\n",
    "<br>LR test multiclass LogLoss = 0.2792\n",
    "<br>CB test multiclass LogLoss = 0.2505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно, если поподбирать параметры, произойдет какое-то улучшение, но не понятно, существенное ли.\n",
    "<br>Забавно, что в данном случае LR == CB, видимо, слишком переупростили данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Еще чуть-чуть в глубь теории SVD**\n",
    "\n",
    "Вкратце еще раз, как работает низкоранговое приближение $\\left\\lVert A - B \\right\\rVert^{2} \\rightarrow \\min\\limits_{B}$.\n",
    "<br>Сначала находим через SVD $A = U S V^{T}$. Потом надо найти матрицу $B$ ранга $m \\leq n$, где $rk A = n$. Тогда в матрице $S$ останутся только первые наибольшие $m$ сингулярных значений, а остальные занулятся. Т.е. получу $S_m$. В итоге $B = U S_m V^{T}$. И если $A \\in M_{p, q}$, то B тоже будет $\\in M_{p, q}$.\n",
    "\n",
    "<br>Возвращаясь к тому, что я хочу сделать: а именно, зафиллить miss'ы у $A$. Т.е. в теории хочу получить такую матрицу $B$, что где у $A$ miss'ов нет, то значения $A$ и $B$ совпадают, а где miss'ы есть, то значения из $B$ подставить взамен miss'ов $A$. \n",
    "<br>Но проблема в том, что низкоранговое SVD дает мне какую-то матрицу $B$, в том плане, что на ней лишь достигается $\\min \\left\\lVert A - B \\right\\rVert^{2}$, но значения вообще могут со значениями $A$ не пересекаться. Информация из $A$ сохраняется только такая, \"спектральная\". В итоге это дело может невообразимо изменить исходное признаковое пространство. Поэтому, например, для рекомендация нам все равно, если мы от $A$ ни оставили ни одного исходного значения на том же месте, но для заполнения miss'ов нам абсолютно не все равно.\n",
    "\n",
    "<br>Это я к чему. ЧЕРЕЗ ПРОСТО SVD MISS'Ы НЕ ЗАПОЛНИТЬ.\n",
    "\n",
    "--\n",
    "<br>Вообще говоря, возвращаясь к теории, при различных сингулярных (даже только при $m$, $m+1$) значения низкоранговое приближение почти единственное. \n",
    "\n",
    "Но ситуацию это не спасает по той причине, что SVD НЕ может работать с матрицами, содержащими NAs. В итоге, мы в лучше случае восстановим лишь почти что матрицу $\\hat{A}$, которую уже заполнили mean'ми. Но нам нужно ее заполнить, а не восстановить mean'ированную с отличием лишь в знаках.\n",
    "<br>--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaps filling: part V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least Square\n",
    "\n",
    "Хотим провести для матрицы $A$ разложение (matrix factorization) через матрицу $X$ или низкоранговое приближение $\\left\\lVert A - X \\right\\rVert_{*}^{*} \\rightarrow \\min\\limits_{X}$, чтобы это минимизировало некоторый функционал потерь на матрицах $rk(X) \\leq r \\leq \\min\\{m, n\\}$. Например: $J(X) = \\left\\lVert A - X \\right\\rVert_{F}^{2} + \\lambda \\left\\lVert X \\right\\rVert_{F}^{2}, \\ Q: {\\rm I\\!R}^{m \\times n} \\rightarrow {\\rm I\\!R}$.\n",
    "\n",
    "<br>Для задачи о наилучшем приближении матрицы A ранга $r$ уже знаем, что SVD является лучшим приближнием A по нормам фробениусовой, $\\ell_{2}$ и, <br>на самом деле, по любой унитарно-инвариантной норме.\n",
    "<br>// *Норма на* $\\mathbb{C}^{m \\times n}$ *является унитарно-инвариантной, если $\\left\\lVert U A V \\right\\rVert$ = $\\left\\lVert A \\right\\rVert \\ \\ \\forall \\ \\text{унитарных} \\ U \\in \\mathbb{C}^{m \\times m}, \\ V \\in \\mathbb{C}^{n \\times n} \\ \n",
    "\\& \\ \\forall A \\in \\mathbb{C}^{m \\times n}$.*\n",
    "<br>// *$U \\in \\mathbb{C}^{m \\times n}$ - унитарная матрица, если $U U^{*} = E$ или (что тоже самое) $U^{-1} = U^{*} = \\overline{U^{T}}$.*\n",
    "\n",
    "Но на практике функционалы могут быть другими (и более сложными). Например: \n",
    "$$ Q(X) = \\left\\lVert P_{\\Omega} \\circ \\left(A - X\\right) \\right\\rVert_{F}^{2}, \\quad  P_{\\Omega} = I_{(i, j) \\in \\Omega} \\quad \\circ - \\text{поэлементное произведение} $$\n",
    "По сути, матрица $A$ известна только в некоторых позициях и там происходит умножение на $1$, а где не известна, умножение на $0$, в итоге фробениусовая норма не портится. Получаем задачу восстановления матрицы $A$ по наблюдаемым элементам: *matrix completion problem*.\n",
    "\n",
    "Для $A = \\begin{pmatrix} \\text{NA} & \\text{NA} & 5 \\\\ 3 & 2 & 2 \\\\ 5 & \\text{NA} & 5 \\end{pmatrix}$ имеем \n",
    "$P_{\\Omega} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix}$.\n",
    "\n",
    "<br>Вспомним, что для всякой матрицы $X$ порядка $m \\times n$ ранга $r$ существует представление в виде произведения двух матриц $U$ и $V^T$, <br>где $U$ порядка $m \\times r$, $V^{T}$ порядка $r \\times n$, и их ранги равны $r$ (*скелетное разложение*).\n",
    "\n",
    "В итоге, при выбранной норме функционала, приходим к следующей постановке задачи:\n",
    "$$J \\left( U, \\ V \\right) = \\left\\lVert A - U V^{T} \\right\\rVert_2 + \\lambda \\left( \\left\\lVert U \\right\\rVert_2 + \\left\\lVert V \\right\\rVert_2 \\right) \\rightarrow \\min\\limits_{U, \\ V}$$\n",
    "Первое слагаемое в чистом виде MSE - мера расстояния между исходной матрицей $A$ и ее аппроксимацией; второе - regularization term.\n",
    "<br>Первое слагаемое есть функция затрат (потерь) $Q$: \n",
    "$$Q \\left( U, \\ V \\right) = \\left\\lVert A - U V^{T} \\right\\rVert_2 = \\sum\\limits_{i, j} \\left( A_{ij} - U_i V_j^T \\right)$$\n",
    "Такой объект не является выпуклым из-за члена $U_i V_j^{T}$; даже больше, такая задача является NP-сложной. Здесь можно использовать градиентный спуск в качестве приблизительного подхода, однако он оказывается медленным и требует большого количества итераций.\n",
    "<br>Обратим внимание, что если мы зафиксируем набор переменных $V$ и будем рассматривать их как константы, то функционал будет выпуклой функцией $U$ и наоборот. Т.е. будем получать простую задача линейной регрессии, которая решается по методу наименьших квадратов (ordinary least squares). \n",
    "<br>// *Для $\\left\\lVert y - X \\beta \\right\\rVert, \\ y, \\ X \\ \\text{- fix}$,  имеем: $\\beta = \\left(X^{T} X\\right)^{-1} X^{T} y$.*\n",
    "\n",
    "<br>**Идея**: по сути, ALS делает это же, но только в несколько этапов. Такой вот двухшаговый итерационный процесс: сначала фиксируется $V$ и решается задача оптимизации по $U$, затем фиксируется найденное $U$ и решается задача по $V$. Так как решение МНК (OLS) единственное и гарантирует минимум MSE, на каждом шаге функционал качества $J$ может либо убывать, либо оставаться неизменным, но никогда возрастать. Чередование двух шагов гарантирует не увеличение функции потерь $Q$, пока не сойдется, но, правда, только к локальному минимуму. Все это также очень зависит от начальных приближений $U_0$, $V_0$ в этом итерационном процессе.\n",
    "\n",
    "<br>---\n",
    "\n",
    "<br>**Алгоритм (ALS vanilla; 1994, Paatero, Tapper)**:\n",
    "<br>Пусть $U_0, V_0$ - начальное приближение (например, случайные матрицы). Тогда \n",
    "$$U_{k+1} = arg\\,\\min_{U} Q(UV_{k}^{T}) \\\\\n",
    "V_{k+1} = arg\\,\\min_{V} Q(U_{k+1} V^{T})$$\n",
    "\n",
    "*Замечание*: минимизируя эти функционалы по $U$ и по $V$, может так произойти (особенно, если решение имеет rk $< r$), что столбцы $U$ или $V$ будут становиться все более и более мультиколлинеарными, что будет приводить к проблемам численной оптимизации.\n",
    "\n",
    "Давайте дополнительно хотеть ортоганолизировать столбцы $U$ и $V$, чтобы не возникало мультиколлинеарности.\n",
    "\n",
    "<br>**Алгоритм (ALS with orthogonalization)**:\n",
    "<br>Пусть $U_0, V_0$ - начальное приближение (например, случайные матрицы). Тогда \n",
    "$$\\hat{U}_{k+1} = arg\\,\\min_{U} Q(U V_k^T) \\\\\n",
    "\\hat{U}_{k+1} = Q_{k+1} R_{k+1} \\quad \\text{// QR-разложение, ортогонализуем //} \\\\\n",
    "\\hat{V}_{k+1} = arg\\,\\min_{V} Q(Q_{k+1} R_{k+1} V^T) = arg\\,\\min_{V} Q(Q_{k+1} \\hat{V}^{T}) \\\\\n",
    "\\hat{V}_{k+1} = Q^{k+1} R^{k+1}$$ \n",
    "В итоге имеем:\n",
    "$$U_{k+1} = Q_{k+1} \\left(R^{k+1}\\right)^{T} \\\\\n",
    "V_{k+1} = Q^{k+1}$$\n",
    "<br>При QR-разложении в вещественном случае матрица $Q$ будет ортогональной.\n",
    "\n",
    "<br>Вообще говоря, еще у нас есть член регуляризации. Но его на каждом шаге можно разбить на подслагаемые и решать задачу оптимизации относительно новых функций потерь.\n",
    "\n",
    "**Алгоритм (ALS with regularization)**:\n",
    "<br>Пусть $U_0, V_0$ - начальное приближение (например, случайные матрицы). Тогда \n",
    "$$\\forall U_i \\ \\ J(U_i) = \\left\\lVert A_i - U_i V^{T} \\right\\rVert_2 + \\lambda \\left\\lVert U_i \\right\\rVert_2 \\\\\n",
    "\\forall V_j \\ \\ J(V_j) = \\left\\lVert A_j - U V_{j}^{T} \\right\\rVert_2 + \\lambda \\left\\lVert V_j \\right\\rVert_2$$\n",
    "По OLS имеем:\n",
    "$$U_i^* = \\left( V^T V + \\lambda E \\right)^{-1} V^{T} A_{i} \\\\\n",
    "V_j^* = \\left( U^T U + \\lambda E \\right)^{-1} U^{T} A_{j}$$\n",
    "\n",
    "*Замечание*: так как $U_i^*$ ($V_j^*$) не зависит напрямую (но через $V$ ($U$)) от $U_{j \\neq i}$ ($V_{i \\neq j}$), то этот процесс распараллеливается.\n",
    "<br>---\n",
    "\n",
    "<br>**QR-разложение**:\n",
    "<br>Матрица $A$ размера $n \\times n$ с комплексными элементами может быть представлена в виде:\n",
    "$$A = Q R$$\n",
    "где $Q$ — унитарная матрица размера $n \\times n$, а $R$ — верхнетреугольная матрица размера $n \\times n$.\n",
    "\n",
    "*Замечания*: \n",
    "<br>$1$. Если $A$ - квадратная невырожденная матрица, то существует единственное QR-разложение, если наложить дополнительное условие, что элементы на диагонали матрицы $R$ должны быть положительными вещественными числами.\n",
    "<br>$2$. Если у $A$ первые $k$ столбцов линейно независимы, тогда первые $k$ столбцов матрицы $Q$ образуют ортогональный базис для этих столбцов матрицы $A$. \n",
    "<br>$3$. Тот факт, что любой столбец $K$ из А зависит только от первых $k$ столбцов $Q$ отвечает за треугольную форму $R$.\n",
    "\n",
    "<br>---\n",
    "\n",
    "<br>Когда мы хотим заполнить пропущенные значения при помощи матричного разложения, следует изменить функционал потерь:\n",
    "$$\\left\\lVert P_{\\Omega} \\circ \\left(A - U V^T\\right) \\right\\rVert_{F}^{2} + \\lambda \\left( \\left\\lVert U \\right\\rVert_{F}^{2} + \\left\\lVert V \\right\\rVert_{F}^{2} \\right) = \\sum\\limits_{i, j} p_{i j} \\left(A_{i j} - U_i V_j^T \\right)^{2} + \\lambda \\left( \\left\\lVert U \\right\\rVert_{F}^{2} + \\left\\lVert V \\right\\rVert_{F}^{2} \\right)$$\n",
    "где $p_{i j} = I_{A_{i j} - \\text{известно}}$. Тогда решением МНК будет:\n",
    "$$U_i^* = \\left( V^T p_i V + \\lambda E \\right)^{-1} V^{T} p_i A_{i} \\\\\n",
    "V_j^* = \\left( U^T p_j U + \\lambda E \\right)^{-1} U^{T} p_j A_{j}$$\n",
    "\n",
    "$P_{\\Omega}$ не зануляет неизвестные элементы $A$. Мы лишь говорим, что при подсчете фробениусовой нормы, вклад от неизвестных элементов матрицы $A$ в эту норму будет нулевой. Если хотим делать малоранговое приближение, то на матрицу $X$ ($= U V^T$) нужно дополнительно накинуть ограничение (условие) малоранговости.\n",
    "<br>---\n",
    "\n",
    "<br>**Еще про ALS**.\n",
    "\n",
    "При восстановлении пропущенных значений мы так же можем использовать не только матрциу $P_{\\Omega}$, но и добавить новую матрицу $C_{\\Omega}$. \n",
    "<br>Пусть с этого момента $p_{i j}$ будет называться *предпочтением* (*preference*), а $c_{i j}$ - *уверенностью* (*confidence*). \n",
    "<br>Новое решение состоит в том, чтобы объединить предпочтение (p) для элемента с уверенностью (c), которая у нас есть для этого предпочтения. <br>Мы начинаем с пропущенных значений как отрицательного предпочтения с низким значением достоверности и существующих значений как положительного предпочтения, но с высоким значением достоверности. Для задач коллаборативной фильтрации можно использовать что-то вроде количества воспроизведений, времени, проведенного на странице или какой-либо другой формы взаимодействия, в качестве основы для расчета уверенности.\n",
    "$$p_{i j} = I_{a_{i j} \\ известен} \\\\\n",
    "c_{i j} = 1 + \\alpha p_{i j}$$\n",
    "\n",
    "Здесь доверие рассчитывается с использованием величины $p$ (данные обратной связи), что дает нам большую уверенность, чем больше раз, <br>в случае коллаборативной фильтрации (как пример), пользователь играл, просматривал или щелкал элемент. Скорость увеличения нашей уверенности задается линейным масштабным коэффициентом $\\alpha$. Мы также добавляем $1$, чтобы получить минимальную уверенность, даже если $\\alpha \\times p$ равно нулю.\n",
    "<br>Это также означает, что даже если у нас будет только одно взаимодействие между пользователем и элементом, достоверность будет выше, чем достоверность неизвестных данных с учетом значения $\\alpha$. В [статье](http://yifanhu.net/PUB/cf.pdf) было обнаруженно, что $\\alpha = 40$ работает хорошо.\n",
    "\n",
    "<br>Теперь цель состоит в том, чтобы найти вектор $\\forall \\, U_i$ и $\\forall \\, V_j$ в размерах функций, что означает, что мы хотим минимизировать следующую функцию потерь:\n",
    "$$\\min_{U*, \\ V*} \\sum\\limits_{i j} c_{i j} \\left( p_{i j} - U_i V_j^{T} \\right)^2 + \\lambda (\\left\\lVert U_i \\right\\rVert_2 + \\left\\lVert V_j \\right\\rVert_2)$$\n",
    "Как отмечается в этой же статье, если мы зафиксируем векторы $U$ или векторы $V$, то сможем вычислить *глобальный* минимум. Производная от приведенного выше уравнения дает нам следующее уравнение для минимизации потерь:\n",
    "$$U_i = \\left( V^{T} C^{i} V + \\lambda E \\right)^{-1} V^{T} C^{i} p(i) \\\\\n",
    "V_j = \\left( U^{T} C^{j} U + \\lambda E \\right)^{-1} U^{T} C^{j} p(j)$$\n",
    "Еще один шаг заключается в том, что, понимая, что произведение $V^{T}$, $C^i$ и $V$ можно разделить следующим образом:\n",
    "$$V^{T} C^{i} V = V^{T} V + V^{T} \\left( C^{i} - E \\right) V$$\n",
    "Теперь имеем $V^T V$ и $U^{T} U$, независимые от $i$ и $j$, что означает, что можно предварительно вычислить их и сделать вычисления значительно менее интенсивными. Имея это в виду, окончательные уравнения:\n",
    "$$U_i = \\left( V^T V + V^{T} \\left( C^i - E \\right) V + \\lambda E \\right)^{-1} V^{T} C^i p(i) \\\\\n",
    "V_j = \\left( U^T U + U^{T} \\left( C^j - E \\right) U + \\lambda E \\right)^{-1} U^{T} C^j p(j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем что-то делать, проверим, совпадают ли размерности столбцов в наших данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train.shape[1] == test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, совпадают ли размерности числовых и категориальных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "numeric_train_cols = list(train.select_dtypes(include=numerics).columns)\n",
    "string_train_cols = list(train.select_dtypes(include='object').columns)\n",
    "\n",
    "numeric_test_cols = list(test.select_dtypes(include=numerics).columns)\n",
    "string_test_cols = list(test.select_dtypes(include='object').columns)\n",
    "\n",
    "print(len(numeric_train_cols) == len(numeric_test_cols), \n",
    "      len(string_train_cols) == len(string_test_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все хорошо. Можно приступать к тюнингу данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_als = train.copy()\n",
    "test_als = test.copy()\n",
    "sorted(train.columns) == sorted(train_als.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1188)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = train_als.isnull().sum() == 8000\n",
    "cols_full_na = list(train_als.columns[mask])\n",
    "\n",
    "mask = test_als.isnull().sum() == 8000\n",
    "cols_full_na_test = list(test_als.columns[mask])\n",
    "\n",
    "train_als = train_als.drop(cols_full_na, axis=1)\n",
    "train_als.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>n_0000</th>\n",
       "      <th>n_0001</th>\n",
       "      <th>n_0002</th>\n",
       "      <th>n_0003</th>\n",
       "      <th>n_0004</th>\n",
       "      <th>n_0005</th>\n",
       "      <th>n_0006</th>\n",
       "      <th>n_0007</th>\n",
       "      <th>n_0009</th>\n",
       "      <th>...</th>\n",
       "      <th>c_1366</th>\n",
       "      <th>c_1367</th>\n",
       "      <th>c_1369</th>\n",
       "      <th>c_1370</th>\n",
       "      <th>c_1372</th>\n",
       "      <th>c_1373</th>\n",
       "      <th>c_1374</th>\n",
       "      <th>c_1375</th>\n",
       "      <th>c_1376</th>\n",
       "      <th>c_1377</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>q</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>b</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  release  n_0000  n_0001    n_0002  n_0003  n_0004    n_0005  n_0006  n_0007  \\\n",
       "0       a     NaN     NaN  0.025449     NaN     NaN  0.368421     NaN     NaN   \n",
       "1       a     NaN     NaN  0.031297     NaN     NaN  0.315789     NaN     NaN   \n",
       "2       a     NaN     NaN  0.024475     NaN     NaN  0.342105     NaN     NaN   \n",
       "3       a     NaN     NaN  0.041694     NaN     NaN  0.447368     NaN     NaN   \n",
       "4       c     NaN     NaN  0.038120     NaN     NaN  0.315789     NaN     NaN   \n",
       "\n",
       "   n_0009  ...  c_1366  c_1367  c_1369  c_1370  c_1372  c_1373  c_1374  \\\n",
       "0     NaN  ...                                       a               q   \n",
       "1     NaN  ...                               a       a                   \n",
       "2     NaN  ...                               a       a               b   \n",
       "3     NaN  ...                                       a                   \n",
       "4     NaN  ...                               b       a               a   \n",
       "\n",
       "   c_1375  c_1376  c_1377  \n",
       "0                          \n",
       "1                          \n",
       "2                          \n",
       "3                          \n",
       "4                          \n",
       "\n",
       "[5 rows x 1188 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_als[string_train_cols] = train_als[string_train_cols].fillna('')\n",
    "\n",
    "train_als.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 6491)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_als_ohe = OneHotEncoder(handle_unknown=\"ignore\").fit_transform(train_als[string_train_cols])\n",
    "train_als_ohe = pd.DataFrame.sparse.from_spmatrix(train_als_ohe)\n",
    "\n",
    "train_als = train_als.select_dtypes(include=numerics)\n",
    "train_als = pd.concat([train_als, train_als_ohe], axis=1, join=\"inner\")\n",
    "train_als.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ищем ALS для нашей обработанной матрицы.\n",
    "<br>// Хотя уже не хорошо, что categorial признаки пришлось зафиллить, иначе OHE не работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "from scipy.sparse import coo_matrix    ## - support efficient modification; stores a list of (row, column, value) tuples\n",
    "from scipy.sparse import csr_matrix    ## - support efficient access and matrix operations; represents a matrix M by three \n",
    "                                       ##   (one-dimensional) arrays, that respectively contain nonzero values, \n",
    "                                       ##   the extents of rows, and column indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8000x6491 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9443612 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coo = coo_matrix(train_als.values)\n",
    "df_coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d861da75a84a74bbd28e83e4ff4bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ModelFitError",
     "evalue": "NaN encountered in factors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModelFitError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a7499ca74533>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# train the model on a sparse matrix of item/user/confidence weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_coo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"total time: {datetime.utcnow() - start}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\maximcha\\anaconda\\lib\\site-packages\\implicit\\als.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, item_users, show_progress)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Final training loss %.4f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit_gpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCiu_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCui_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mimplicit/recommender_base.pyx\u001b[0m in \u001b[0;36mimplicit.recommender_base.MatrixFactorizationBase._check_fit_errors\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mModelFitError\u001b[0m: NaN encountered in factors"
     ]
    }
   ],
   "source": [
    "# initialize a model\n",
    "model = AlternatingLeastSquares(factors=40, iterations=100, calculate_training_loss=True) ## loss=nan всегда отображ здесь\n",
    "\n",
    "# train the model on a sparse matrix of item/user/confidence weights\n",
    "start = datetime.utcnow()\n",
    "model.fit(df_coo)  \n",
    "print(f\"total time: {datetime.utcnow() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я просто поражаюсь, что ALS'у уже ~30 лет, проблема miss'ов в рекомендациях давнишняя, но они не смогли реализовать нормально алгоритм заполнения miss'ов для заполнения miss'ов -- упал на miss'ах. Аплодирую стоя.\n",
    "\n",
    "Охх, все беды от работы с чужим кодом.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8000x6491 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7573912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_als = train_als.fillna(0)\n",
    "df_csr = csr_matrix(train_als.values)\n",
    "df_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fe685ec96943019dc650181c92d868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total time: 0:00:56.057166\n"
     ]
    }
   ],
   "source": [
    "# initialize a model\n",
    "model = AlternatingLeastSquares(factors=40, iterations=100, calculate_training_loss=True) ## loss=nan всегда отобр здесь\n",
    "\n",
    "# train the model on a sparse matrix of item/user/confidence weights\n",
    "start = datetime.utcnow()\n",
    "model.fit(df_csr)  \n",
    "print(f\"total time: {datetime.utcnow() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не знаю, как реализован als точно в этой библиотеке, но что сейчас, что до этого я игрался, loss ~ за итераций 10 доходит до локального экстремума и все. Это невероятно. Причем игрался я на очень классных матрицах, а loss у als сходился всегда довольно быстро к фигне какой-то в целом. Неужели за 30 лет никто не сделал норм либу по ALS..\n",
    "\n",
    "Я, конечно не совсем права, раз в 200 итераций loss все же изменяется на целую тысячную!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6481</th>\n",
       "      <th>6482</th>\n",
       "      <th>6483</th>\n",
       "      <th>6484</th>\n",
       "      <th>6485</th>\n",
       "      <th>6486</th>\n",
       "      <th>6487</th>\n",
       "      <th>6488</th>\n",
       "      <th>6489</th>\n",
       "      <th>6490</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1         2     3     4         5     6     7     8     9     ...  \\\n",
       "0   0.0   0.0  0.025449   0.0   0.0  0.368421   0.0   0.0   0.0   0.0  ...   \n",
       "1   0.0   0.0  0.031297   0.0   0.0  0.315789   0.0   0.0   0.0   0.0  ...   \n",
       "2   0.0   0.0  0.024475   0.0   0.0  0.342105   0.0   0.0   0.0   0.0  ...   \n",
       "3   0.0   0.0  0.041694   0.0   0.0  0.447368   0.0   0.0   0.0   0.0  ...   \n",
       "4   0.0   0.0  0.038120   0.0   0.0  0.315789   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "   6481  6482  6483  6484  6485  6486  6487  6488  6489  6490  \n",
       "0   0.0   0.0   0.0   0.0   1.0   0.0   0.0   1.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   1.0   0.0   0.0   1.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   1.0   0.0   0.0   1.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   1.0   0.0   0.0   1.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   1.0   0.0   0.0   1.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 6491 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.sparse.from_spmatrix(df_csr).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 40)\n",
      "(6491, 40)\n"
     ]
    }
   ],
   "source": [
    "print(model.item_factors.shape, model.user_factors.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7306, 0.010278553),\n",
       " (4967, 0.00992531),\n",
       " (7282, 0.009865671),\n",
       " (4806, 0.009752322),\n",
       " (6458, 0.009576814),\n",
       " (1257, 0.009567821),\n",
       " (5254, 0.009473771),\n",
       " (2329, 0.009408616),\n",
       " (2917, 0.009396792),\n",
       " (2984, 0.009393851)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommend items for a user\n",
    "df_csr = df_csr.T.tocsr()\n",
    "df_csr_fitted = model.recommend(0, df_csr)\n",
    "df_csr_fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это пичаль."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще я игрался с таким, приводя матрицу к 3м столбцам: ```items```, ```users```, ```rating```.\n",
    "<br>Но прямо скажем, что какие-то танцы с бубнами (что для зафилливания мне приходится изменять полностью вид)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.melt(train_als, ignore_index = False)\n",
    "df_train['index'] = [i for i in range(0, train_als.shape[0])] * train_als.shape[1]\n",
    "df_train.dropna()\n",
    "\n",
    "# В конце нужно было бы сделать так\n",
    "df_train_original = df_train.pivot(columns=\"variable\", values=\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но какого-то фига, даже если захотеть пошаманить, этот ALS полностью изменял матрицу исходную (train_als) -- это вообще как? <br>Какой ALS они используют..\n",
    "\n",
    "Вывод: писать ALS самому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
